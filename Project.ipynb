{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "from keras import models, layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Concatenate, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"concatenated_file.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop non-relevant columns\n",
    "if \"sample_id\" in df.columns:\n",
    "    df.drop(columns=[\"sample_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaN values\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "# Encode severity as numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"severity\"] = label_encoder.fit_transform(df[\"severity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = df.drop(columns=[\"severity\"]).values\n",
    "y = to_categorical(df[\"severity\"].values)  # For model training\n",
    "\n",
    "# Normalize features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape X to fit Conv1D input (samples, time steps, features)\n",
    "X = X[..., np.newaxis]  # Add dimension for Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LeNet-style CNN model (1D)\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "num_classes = y.shape[1]\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    # First convolution + average pooling\n",
    "    layers.Conv1D(filters=6, kernel_size=5, padding='same', activation='sigmoid'),\n",
    "    layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "    # Second convolution + average pooling\n",
    "    layers.Conv1D(filters=16, kernel_size=5, activation='sigmoid'),\n",
    "    layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "    # Fully connected layers\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='sigmoid'),\n",
    "    layers.Dense(84, activation='sigmoid'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Conv1D(filters=16, kernel_size=11, strides=4, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3, strides=2),\n",
    "    layers.Conv1D(filters=24, kernel_size=5, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3, strides=2),\n",
    "    layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3, strides=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inception Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inception block for 1D data with regularization\n",
    "def inception_1d(x,\n",
    "                 filters_1x1,\n",
    "                 filters_3x3_reduce,\n",
    "                 filters_3x3,\n",
    "                 filters_5x5_reduce,\n",
    "                 filters_5x5,\n",
    "                 filters_pool):\n",
    "    path1 = layers.Conv1D(filters_1x1, kernel_size=1, padding='same', activation='relu',\n",
    "                          kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "    path2 = layers.Conv1D(filters_3x3_reduce, kernel_size=1, padding='same', activation='relu',\n",
    "                          kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "    path2 = layers.Conv1D(filters_3x3, kernel_size=3, padding='same', activation='relu',\n",
    "                          kernel_regularizer=regularizers.l2(0.01))(path2)\n",
    "\n",
    "\n",
    "    path3 = layers.Conv1D(filters_5x5_reduce, kernel_size=1, padding='same', activation='relu',\n",
    "                          kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "    path3 = layers.Conv1D(filters_5x5, kernel_size=5, padding='same', activation='relu',\n",
    "                          kernel_regularizer=regularizers.l2(0.01))(path3)\n",
    "\n",
    "\n",
    "    path4 = layers.MaxPooling1D(pool_size=3, strides=1, padding='same')(x)\n",
    "    path4 = layers.Conv1D(filters_pool, kernel_size=1, padding='same', activation='relu',\n",
    "                          kernel_regularizer=regularizers.l2(0.01))(path4)\n",
    "\n",
    "\n",
    "    return layers.Concatenate()([path1, path2, path3, path4])\n",
    "\n",
    "#%%\n",
    "# Define the model\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "num_classes = y.shape[1]\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "# First Inception block\n",
    "x = inception_1d(input_layer,\n",
    "                 filters_1x1=16,\n",
    "                 filters_3x3_reduce=8,\n",
    "                 filters_3x3=16,\n",
    "                 filters_5x5_reduce=8,\n",
    "                 filters_5x5=16,\n",
    "                 filters_pool=8)\n",
    "x = layers.Dropout(0.4)(x)  # Increased dropout\n",
    "\n",
    "# Auxiliary classifier\n",
    "aux1 = layers.AveragePooling1D(pool_size=5, strides=3, padding='valid')(x)\n",
    "aux1 = layers.Conv1D(832, kernel_size=1, padding='same', activation='relu')(aux1)\n",
    "aux1 = layers.Flatten()(aux1)\n",
    "aux1 = layers.Dense(832, activation='relu')(aux1)\n",
    "aux1 = layers.Dropout(0.4)(aux1)\n",
    "aux1 = layers.Dense(num_classes, activation='softmax', name='aux_output')(aux1)\n",
    "\n",
    "# Second Inception block\n",
    "x = inception_1d(x,\n",
    "                 filters_1x1=16,\n",
    "                 filters_3x3_reduce=8,\n",
    "                 filters_3x3=16,\n",
    "                 filters_5x5_reduce=16,\n",
    "                 filters_5x5=16,\n",
    "                 filters_pool=8)\n",
    "x = layers.Dropout(0.4)(x)  # Increased dropout\n",
    "\n",
    "# Main output\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.4)(x)  # Increased dropout\n",
    "output_layer = layers.Dense(num_classes, activation='softmax', name='main_output')(x)\n",
    "\n",
    "# Create model with two outputs\n",
    "model = models.Model(inputs=input_layer, outputs=[output_layer ,aux1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LeNet and AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "model.compile(loss=[\"categorical_crossentropy\", \"categorical_crossentropy\"],\n",
    "             loss_weights=[0.8, 0.2],\n",
    "             optimizer=optimizer,\n",
    "             metrics=[\"categorical_accuracy\",\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train For leNet and AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)\n",
    "\n",
    "# Add callback to your fit method\n",
    "history = model.fit(X_train, y_train, batch_size=30, epochs=10, validation_split=0.1, callbacks=[early_stopping] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with Early Stopping\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "early_stopping = EarlyStopping(monitor='val_main_output_loss', patience=5, restore_best_weights=True, mode='min')\n",
    "history = model.fit(X_train, [y_train, y_train], batch_size=30, epochs=20, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Evaluation + graphs ) for LeNet and AlexNe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "test_loss = score[0]   # Adjust loss as per original code\n",
    "test_accuracy = score[1]  # Adjust accuracy as per original code\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot Loss & Accuracy curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history.get('loss', []), label='Loss Entraînement', color='blue')\n",
    "plt.plot(history.history.get('val_loss', []), label='Loss Validation', color='red')\n",
    "plt.axhline(y=test_loss, color='green', linestyle='--', label='Loss Test')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Perte (Loss)')\n",
    "plt.title('Évolution de la Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history.get('categorical_accuracy', []), label='Accuracy Entraînement', color='blue')\n",
    "plt.plot(history.history.get('val_categorical_accuracy', []), label='Accuracy Validation', color='red')\n",
    "plt.axhline(y=test_accuracy, color='green', linestyle='--', label='Accuracy Test')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Précision (Accuracy)')\n",
    "plt.title(\"Évolution de l'Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Save and display plots\n",
    "#plt.savefig('training_curves_no_normalization.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model and plot Loss & Accuracy curves\n",
    "# Évaluer le modèle sur les données de test\n",
    "score = model.evaluate(X_test, [y_test, y_test], verbose=0)\n",
    "\n",
    "# Afficher les résultats de l'évaluation (using main output metrics)\n",
    "print(\"Test loss:\", score[2])  # Affiche la perte sur l'ensemble de test (combined loss)\n",
    "print(\"Test accuracy:\", score[4])  # Affiche la précision sur l'ensemble de test (main_output accuracy)\n",
    "\n",
    "print(score)\n",
    "# Calculate adjusted test loss and accuracy for plotting\n",
    "test_loss = score[2]   # Adjust loss as per original code\n",
    "test_accuracy = score[4]   # Adjust accuracy as per original code\n",
    "\n",
    "# Plot Loss & Accuracy curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss Curve (using main_output loss)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history.get('main_output_loss', []), label='Loss Entraînement', color='blue')\n",
    "plt.plot(history.history.get('val_main_output_loss', []), label='Loss Validation', color='red')\n",
    "plt.axhline(y=test_loss, color='green', linestyle='--', label='Loss Test')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Perte (Loss)')\n",
    "plt.title('Évolution de la Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy Curve (using main_output categorical_accuracy)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history.get('main_output_categorical_accuracy', []), label='Accuracy Entraînement', color='blue')\n",
    "plt.plot(history.history.get('val_main_output_categorical_accuracy', []), label='Accuracy Validation', color='red')\n",
    "plt.axhline(y=test_accuracy, color='green', linestyle='--', label='Accuracy Test')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Précision (Accuracy)')\n",
    "plt.title(\"Évolution de l'Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Save and display plots\n",
    "#plt.savefig('training_curves_inception_adjusted.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
